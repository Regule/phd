\chapter{Machine learning based approach}


%====================================================================================================
\section{Concepts in machine learning}
Aim of this section is to define what exactly is understood by a term machine learning as well
as to introduce basic concepts that are common for all machine learning applications

%----------------------------------------------------------------------------------------------------
\subsection{Differences between knowledge and data based models}
While creating a model of physical processes one of two approaches may be used.
First, is the knowledge-driven model that as the name implies relays on knowledge of underlying
laws of physics that govern the process.
The main advantage of this approach is its high explainability and reliability as every parameter
of the model corresponds with some specific physical property.
However this model has a very significant downside, it requires detailed knowledge of physical
phenomena. This means that for complex processes it might be very difficult or even
impossible, at a~given moment, to create a model. Another drawback is that the complexity of
the process grows the computational cost of its model.
If those problem makes creating usable knowledge-based model impossible or economically
non efficient a data-driven approach may be used.
It model used is chosen arbitrarily with little or no relation to underlying physics,
then model parameters are adjusted until it will fit experimental data to the best of its
capability.
The main advantage of a data-driven approach is the lack of required knowledge on process 
inner workings, which allow prediction of events which exact mechanics have not 
yet been discovered.
Another element characteristic in the data-driven approach, that can be either advantageous or
disadvantageous, is the lack of direct correlation between the complexity 
of the process and computational cost of the model.
This is a very welcome trait for complex processes like orbital atomic clock ensembles.
The main disadvantage in comparison with the knowledge-driven approach is its lower reliability and
explainability. This is the main reason why the data-driven approach is usually avoided in
critical implementations. Another significant downside is a requirement of a large amount of
experimental data for model adjustment which means that in many cases this approach simply
cannot be used.
The last thing that must be said about the data-driven approach is that it still requires some
amount of knowledge to work efficiently as the selection of models that will be adjusted to
data requires some assumption. For example, the use of linear regression assumes that the model
is linear.

%----------------------------------------------------------------------------------------------------
\subsection{What is a machine learning algorithm}
Term machine learning describes subset of algorithms that adjusts parameter of other algorithms 
according to data. To provide more formal definition an algorithm can be described as a 
transformation function $\phi_{a}$ such that:
\begin{equation}
	\label{equ:algorithm_general}
	\mathcal{Y}_{a} = \phi_{a}(\mathcal{X},\Theta_{a}),
\end{equation}
where $\mathcal{Y}_{a}$ is set of algorithm responses, $\mathcal{X}$ is set of inputs 
and $\Theta_{a}$ is algorithm parameter set.
Input space $\mathcal{U}_{x}$ and response space $\mathcal{U}_{y}$ can be simply a numerical
space, in which case algorithm is called numerical algorithm, but can also represent more 
abstract concepts. For example in case of database software input space will consist of 
queries while response space will contain information stored in database as well as error 
status in case query was written incorrectly.
Machine learning algorithm, denoted as $\phi_{m}$ can also be described in that general form
however what is special in this case is that input space $\mathcal{U}_{m}$ consists of either :
\begin{equation}
	\label{equ:supervised_input}
	\mathcal{X}_{m} = \{\phi_{a}, \mathcal{X}_{a}, \mathcal{Y}_{a}, \Theta_{a} \},
\end{equation}
in case of supervised learning or just :
\begin{equation}
	\label{equ:supervised_input}
	\mathcal{X}_{m} = \{\phi_{a}, \mathcal{X}_{a}, \Theta_{a} \},
\end{equation}
in case of unsupervised learning.
In both cases response of machine learning algorithm is described as a:
\begin{equation}
	\label{equ:ml_response}
	\mathcal{Y}_{m} = \{\hat{\Theta}_{a}, E_{a}\},
\end{equation}
where $\hat{\Theta}_{a}$ is adjusted set of parameters and $E_{a}$ is value prediction 
error for this new set of parameters.
Aim of machine learning algorithm is to adjust parameters $\Theta_{a}$ of given algorithm 
$\phi_{a}$ in such a way to minimize error $E_{a}$ for given data. In case of supervised learning
a correct response $\mathcal{Y}_{a}$ for inputs is known where in case of unsupervised learning
only input set $\mathcal{X}_{a}$ is known. In order to avoid confusion parameters of machine 
learning algorithm $\Theta_{m}$ will be referred to as a \textit{metaparameters}. 
Exact set of meteaparameters vary between specific algorithms, however there is one 
crucial metaparameter that will appear in every machine learning algorithm.
That metaparameter is error function as it is required for error value calculation and as 
minimization of that value is goal of machine learning it is mandatory to be able to calculate it.

%----------------------------------------------------------------------------------------------------
\subsection{Error function}
As an error function is used for calculation of value that will be minimized by a machine 
learning algorithm correct choice of that function is crucial decision when designing such
algorithm.
Due to scope of this work attention will be given only to error function used in supervised 
learning of numerical algorithms.
Error for single pair of predicted value $\hat{y}$ and actual value $y$ can be described as
distance between them in response space $\mathcal{U}_{y}$:
\begin{equation}
	\label{equ:dist_general}
	E = f_{DIST}(\hat{y},y),
\end{equation}
where $f_{DIST}$ is distance function, alternatively called metric.
Distance function is a function that gives a distance between each pair of point elements of a set.
If a set have a distance function defined it is called a metric space. 
In order to be classified as a metric function $f_{DIST}$ over a Cartesian square of a universe 
$\mathcal{U}$ it must display following properties:
\begin{enumerate}
	\item $f_{DIST} : \mathcal{U} \times \mathcal{U} \to \mathbb{R}^{+}$
	\item identity of indiscernibles : $f_{DIST}(x_{0}, x_{1})=0 \leftrightarrow x_{0} = x_{1}$,
	\item symmetry : $f_{DIST}(x_{0},x_{1})=f_{DIST}(x_{1},x_{0})$,
	\item triangle inequality : $f_{DIST}(x_{0},x_{1}) \leq f_{DIST}(x_{0},x_{2}) +
		f_{DIST}(x_{1},x_{2})$. 
\end{enumerate}

There are many types of metrics however within scope of this work attention will be given only to
distance functions related to $\ell$ norms.
As such error function for single prediction will be described as: 
\begin{equation}
	\label{equ:dist_norm}
	E = \Vert (\hat{y}-y) \Vert_{k},
\end{equation}
where notation $\Vert \bullet \Vert_{k}$ describes $k$ norm of given value.
Norm is a function from a real or complex vector space to the non negative real number space
that describes distance of a point from the origin.
Origin is the point for which values of all dimension are equal to zero, in case of calculating
$\ell$ norm for difference between two points subtrahend is treated as a origin point.
General equation describing a $\ell_{k}$ norm in $n$ dimensional space is:
\begin{equation}
	\label{equ:ell_norm}
	\Vert x \Vert_{k} = (\vert x_{0} \vert^{k} + \vert x_{1} \vert^{k} + \cdots +
	\vert x_{n} \vert^{k})^{1/k},
\end{equation}
where $\vert x \vert$ is an absolute value of $x$.
\begin{figure}[htb] 
	\label{fig:l_norms}
	\centering
	\includegraphics[width=0.5\textwidth]{figures/l_norms}
	\caption{Relation between error and difference in first dimension values with second 
	dimension difference at constant 100}
\end{figure}
\textbf{TODO: DESCRIBE WHY USE GIVEN NORMS IN MORE DETAIL}
Norms usually used in error calculation are $\ell_{1}$ and $\ell_{2}$, due to their frequent use
not only in this field they are given specific names.
Norm $\ell_{2}$ is called Euclidean norm and is equivalent to intuitive understanding of distance
while norm $\ell_{1}$ is called Manhattan distance and is related to distance between points on 
grid.
Euclidean distance is used for calculation of Root Mean Squared Error (RMSE). If we define 
knowledge base $K$ as a set of pairs $(x,y)$ where $x$ is input and $y$ an expected output
while prediction is realised by hypothesis $\hat{y} = h(x,\phi)$ error can be described as:
\begin{equation}
	\label{equ:mse}
	f_{RMSE}(K,h) = \sqrt{\frac{1}{\vert K \vert} \sum_{(x,y) \in K}(h(x,\phi)-y)^2},
\end{equation}
where $\vert K \vert$ is cardinality of knowledge base.
Manhattan distance is in turn used to calculate Mean Average Error (MAE):
\begin{equation}
	\label{equ:mae}
	f_{MAE}(K,h) = \frac{1}{\vert K \vert} \sum_{(x,y) \in K}\vert (h(x,\phi)-y)\vert.
\end{equation}
Those two error values will be used in this work.


%----------------------------------------------------------------------------------------------------
\subsection{Deep learning}
At time when experiments described in this dissertation were conducted a new approach to 
machine learning called Deep Learning (DL) became highly popular.
Due to that popularity term deep learning is often overused and misrepresented marketing 
purposes. In this section an attempt will be made to explain what exactly constitutes as a
deep learning and describe what sort of challenges.
Term deep learning can be applied only to a subset of machine learning algorithms that have
layer based architecture, usually a neural networks.
In such implementation whole data processing algorithm can be described as a transformation:
\begin{equation}
	\label{equ:spaces1}
	\phi = \mathcal{U}_{OBS} \Rightarrow \mathcal{U}_{RES},
\end{equation}
where $\mathcal{U}_{OBS}$ is called observation space and $\mathcal{U}_{RES}$ is response space.
In case of simple single layer system observation space is directly transformed into a 
response space, however in case of multi layer model there are intermediate encoding spaces.
In such case representation spaces are numbered by index of layer which outputs them. 
So in case of $n$ layer system it would be $\{\mathcal{U}_{0}, \mathcal{U}_{1},
\cdots, \mathcal{U}_{n}\}$ where indexes $0$ and $n$ are special as 
$\mathcal{U}_{0}=\mathcal{U}_{OBS}$ and $\mathcal{U}_{n}=\mathcal{U}_{RES}$.
In that case a function transforming observation into a reaction can be described as a 
convolution of multiple encoding function.
\begin{equation}
	\label{equ:spaces_transform}
	\phi = \phi_{1} \otimes \phi_{2} \otimes \cdots \otimes \phi_{n}.
\end{equation}
Transformation $\phi_{n}$ is called output layer, while other transformation are referred to
as a hidden layers as their result is not visible during normal operation of algorithm.
Until recently almost all architectures used a single hidden layer and architectures with more
than 3 hidden layers were almost unheard of.
This is due to three issue that show up in systems with higher amount of layer: signal vanishing,
signal explosion and rise in computation due to high parameter count.


%====================================================================================================
\section{Regression approximation}

%----------------------------------------------------------------------------------------------------
\subsection{Linear regression}
\label{sec:linear_regression}
Linear predictor function is a linear combination of a set of coefficients and explanatory 
variables (independent variables), whose value is used to predict the outcome of a
dependent variable. Linear predictor is described as:
\begin{equation}
	\label{equ:linear_predictior}
	\hat{y} = \Theta_{0} + \Theta_{1}x_{1} + \cdots + \Theta_{n}x_{n},
\end{equation}
where $\hat{y}$ is predicted value,$n$ is feature count, $x_{i}$ is value of i-th feature
and $\Theta_{j}$ is j-th parameter.
Parameter $\Theta_{0}$ is special as it is not corresponding to any feature, that parameter
is called a \textit{bias term} or just \textit{bias}.
Predictor will return value of bias if all inputs are set to zero.
Function that allows prediction is called hypotesis function and is written as $\hat{y} = h(x)$.
With assumption that input vector for such function is $x=[1, x_{1}, \cdots, x_{n}]$ then 
a hypotesis function of a linear predictor can be defined as:
\begin{equation}
	\label{equ:linear_hyp}
	\hat{y} = h_{\Theta}(x) = \Theta^{T}\cdot x.
\end{equation}
To define how well prediction fit actuall data a error function must be selected and then one
of to approaches for minimizing it must be applied. Gradient based approach will be described 
is section \ref{sec:gradient} here a analytical approach based on a normal equation will be shown.
To derieve normal equation a MSE will be used as a error function, if other function would be 
selected a different normal equation would be a result.
In matrix notation hypothesis function can be described as :
\begin{equation}
	\label{equ:linear_hyp_matirx}
	\hat{Y} = h_{\Theta}(X) = \Theta^{T}\cdot X,
\end{equation}
where $X$ is input matrix such that each row represent single observation and each column 
single feature:
\begin{equation}
	\label{equ:input_matrix}
	X_{m\times m}=\left( \begin{array}{c c c c c}
		1& x^{(1)}_{1}& x^{(1)}_{2}& \cdots& x^{(1)}_{n}\\
		1& x^{(2)}_{1}& x^{(2)}_{2}& \cdots& x^{(2)}_{n}\\
		1& x^{(3)}_{2}& x^{(3)}_{2}& \cdots& x^{(3)}_{n}\\
		\vdots&\vdots&\vdots&\ddots&\vdots\\
		1& x^{(m)}_{2}& x^{(m)}_{3}& \cdots& x^{(m)}_{n}\\
	\end{array}\right),
\end{equation}
where $x^{(i)}_{j}$ denotes value of $j$ feature of $i$ observation.
Using this definitions the least-squares error function can be represented in matrix form as:
\begin{equation}
	\label{equ:mse_matrix}
	f_{MSE}(X,y) = \frac{1}{2m}(X\Theta - y)^{T}(X\Theta - y), 
\end{equation}
where $y$ is column vector of expected responses.
For puproses of of minimization the constans scaling factor $\frac{1}{2m}$ can be ignored, it is
guaranteed that $m\neq 0$ as there is no meaning of learning for zero observations.
With that following transformations of equation can be made:
\begin{equation}
	\label{equ:mse_transform1}
	f_{MSE}(X) = ((X\Theta)^{T} - y^{T})^{T}(X\Theta - y), 
\end{equation}
\begin{equation}
	\label{equ:mse_transform2}
	f_{MSE}(X) = (X\Theta)^{T}X\Theta-(X\Theta)^{T}y-y^{T}(X\Theta)+y^{T}y.
\end{equation}
This can be even further simplified to:
\begin{equation}
	\label{equ:mse_transform2}
	f_{MSE}(X) = \Theta^{T}X^{T}X\Theta-2(X\Theta)^{T}y+y^{T}y. 
\end{equation}
As $\theta$ is the unknown that is being minimised $f_{MSE}$ have to be derived by $\theta$
and compared to 0.
Result od that deriviation is:
\begin{equation}
	\label{equ:mse_derived}
	\frac{\partial J}{\partial \Theta} = 2X^{T}X\Theta - 2X^{T}y = 0,
\end{equation}
form that an equation it is simple to derieve a normal equation that allows calculation of 
$\Theta$:
\begin{equation}
	\label{equ:mse_derived}
	\Theta = (X^{T}X)^{-1}X^{T}y.	
\end{equation}


Standard linear regression models with standard estimation techniques make a number of 
assumptions about the predictor variables, the response variables and their relationship.
Numerous extensions have been developed that allow each of these assumptions to be relaxed 
(i.e. reduced to a weaker form), and in some cases eliminated entirely. 
Generally these extensions make the estimation procedure more complex and time-consuming,
and may also require more data in order to produce an equally precise model.

Example of a cubic polynomial regression, which is a type of linear regression. 
Although polynomial regression fits a nonlinear model to the data, as 
a statistical estimation problem it is linear, in the sense that the regression 
function E(y | x) is linear in the unknown parameters that are estimated from the data. 
For this reason, polynomial regression is considered to be a special case of 
multiple linear regression.

The following are the major assumptions made by standard linear regression models with 
standard estimation techniques (e.g. ordinary least squares):

\begin{enumerate}
    \item Weak exogeneity. This essentially means that the predictor variables x 
		can be treated as fixed values, rather than random variables. 
		This means, for example, that the predictor variables are assumed to be error-free—that 
		is, not contaminated with measurement errors. 
		Although this assumption is not realistic in many settings, dropping it leads to
		significantly more difficult errors-in-variables models.
    \item Linearity. This means that the mean of the response variable is a linear combination 
		of the parameters (regression coefficients) and the predictor variables.
		Note that this assumption is much less restrictive than it may at first seem. 
		Because the predictor variables are treated as fixed values (see above), linearity is
		really only a restriction on the parameters. 
		The predictor variables themselves can be arbitrarily transformed, and in fact multiple
		copies of the same underlying predictor variable can be added, each one transformed 
		differently. This technique is used, for example, in polynomial regression, 
		which uses linear regression to fit the response variable as an arbitrary polynomial 
		function (up to a given rank) of a predictor variable.
		With this much flexibility, models such as polynomial regression often have
		"too much power", in that they tend to overfit the data. As a result,
		some kind of regularization must typically be used to prevent unreasonable solutions
		coming out of the estimation process. 
		Common examples are ridge regression and lasso regression. Bayesian linear regression 
		can also be used, which by its nature is more or 
		less immune to the problem of overfitting.
    \item Constant variance (a.k.a. homoscedasticity). This means that the variance of 
		the errors does not depend on the values of the predictor variables.
		Thus the variability of the responses for given fixed values of the predictors is the
		same regardless of how large or small the responses are. 
		This is often not the case, as a variable whose mean is large will typically have a 
		greater variance than one whose mean is small.
\end{enumerate}
To check for violations of the assumptions of lineaarity, constant variance, and independence
of errors within a linear regression model, the residuals are typically plotted against the 
predicted values (or each of the individual predictors). 
An apparently random scatter of points about the horizontal midline at 0 is ideal, 
but cannot rule out certain kinds of violations such as autocorrelation in the errors or their 
correlation with one or more covariates.
Independence of errors. This assumes that the errors of the response variables are uncorrelated 
with each other.
(Actual statistical independence is a stronger condition than mere lack of correlation and is 
often not needed, although it can be exploited if it is known to hold.) 
Some methods such as generalized least squares are capable of handling correlated errors, 
although they typically require significantly more data unless some sort of regularization is used 
to bias the model towards assuming uncorrelated errors. Bayesian linear regression is a general 
way of handling this issue.
Lack of perfect multicollinearity in the predictors. 
For standard least squares estimation methods, the design matrix X must have full column rank p; 
otherwise perfect multicollinearity exists in the predictor variables, meaning a linear 
relationship exists between two or more predictor variables. 
This can be caused by accidentally duplicating a variable in the data, using a linear 
transformation of a variable along with the original 
(e.g., the same temperature measurements expressed in Fahrenheit and Celcius), or including a 
linear combination of multiple variables in the model, such as their mean. 
It can also happen if there is too little data available compared to the number of 
parameters to be estimated (e.g., fewer data points than regression coefficients). 
Near violations of this assumption, where predictors are highly but not perfectly correlated, 
can reduce the precision of parameter estimates (see Variance inflation factor). 
In the case of perfect multicollinearity, the parameter vector $\beta$ will be non-identifiable—it 
has no unique solution. In such a case, only some of the parameters can be identified 
(i.e., their values can only be estimated within some linear subspace of 
the full parameter space Rp). See partial least squares regression. 
Methods for fitting linear models with multicollinearity have been developed,
some of which require additional assumptions such as "effect sparsity"—that a large 
fraction of the effects are exactly zero.
Note that the more computationally expensive iterated algorithms for parameter estimation, 
such as those used in generalized linear models, do not suffer from this problem.

%----------------------------------------------------------------------------------------------------
\subsection{Polynomial regression}
Polynomial regression is one example of regression analysis using basis functions to model 
a functional relationship between two quantities.
More specifically, it uses transformation $\phi$  that transforms observation vector 
$x \in \mathbb{R}$ into  $ \acute{x} \in \mathbb{R}^{n}$ such that :
\begin{equation}
 x \to^{\phi} \acute{x} = \{x^{1}, x^{2}, \cdots, x^{n}\}.
\end{equation}
\begin{figure}[htb] 
	\label{fig:space_transformation}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/space_transformation}
	\caption{Non-linear mapping of input examples into high dimensional feature space}
\end{figure}
A drawback of polynomial bases is that the basis functions are "non-local",
meaning that the fitted value of $\hat{y}$ at a given value $x = x_{0}$ depends strongly on 
data values with $x$ far from $x_{0}$.
In modern statistics, polynomial basis-functions are used along with new basis functions,
such as splines, radial basis functions, and wavelets.
These families of basis functions offer a more parsimonious fit for many types of data. 
Adter transformation new observation space is treated as a input for linear regression.


%----------------------------------------------------------------------------------------------------
\subsection{Support vector machine}
Support Vector Machines are very specific class of algorithms, 
characterized by usage of kernels, absence of local minima, sparseness of the solution and 
capacity control obtained by acting on the margin, or on number of support vectors, etc.

They were invented by Vladimir Vapnik and his co-workers, 
and first introduced at the Computational Learning Theory (COLT) 1992 conference with the paper.
All these nice features however were already present in machine learning since 1960’s: 
large margin hyper planes usage of kernels, geometrical interpretation of kernels as inner
products in a feature space.
Similar optimization techniques were used in pattern recognition and sparsness techniques 
were widely discussed.
Usage of slack variables to overcome noise in the data and non - separability was also
introduced in 1960s.
However it was not until 1992 that all these features were put together to form the
maximal margin classifier, the basic Support Vector Machine, and not until 1995 that the
soft margin version was introduced.

Support Vector Machine can be applied not only to classification problems but also to the
case of regression.
Still it contains all the main features that characterize maximum margin algorithm:
a non-linear function is leaned by linear learning machine mapping into high dimensional kernel
induced feature space.
The capacity of the system is controlled by parameters that do not depend on the dimensionality
of feature space.

In the same way as with classification approach there is motivation to seek and optimize the
generalization bounds given for regression.
They relied on defining the loss function that ignores errors, which are situated within
the certain distance of the true value.
This type of function is often called – epsilon intensive – loss function. 
The figure below shows an example of one-dimensional linear regression function with – 
epsilon intensive – band. The variables measure the cost of the errors on the training points.
These are zero for all points that are inside the band. 
\begin{figure}[htb] 
	\label{fig:svm1}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/svm1}
	\caption{One-dimensional linear regression with epsilon intensive band}
\end{figure}

One of the most important ideas in Support Vector Classification and Regression cases is 
that presenting the solution by means of small subset of training points gives enormous 
computational advantages. 
Using the epsilon intensive loss function we ensure existence of the global minimum and at 
the same time optimization of reliable generalization bound.
\begin{figure}[htb] 
	\label{fig:epsilon_band}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/epsilon_band}
	\caption{Detailed picture of epsilon band with slack variables and selected data points}
\end{figure}
In SVM regression, the inputis first mapped onto a m-dimensional feature space using some 
fixed (nonlinear) mapping, and then a linear model is constructed in this feature space. 
Using mathematical notation, the linear model (in the feature space) is given by:
\begin{equation}
	\label{equ:svm_in}
	h(x) = \sum_{j=1}^{m}\Theta_{j}x_{j} + b
\end{equation}
If the data is assumed to be zero mean bias term $b$ can be dropped which results in following
equation in matrix form:
\begin{equation}
	\label{equ:svm_in_matrix}
	h(x) = \Theta X,
\end{equation}
where $X$ is matrix containing all observations.

The quality of estimation is measured by the loss function.
SVM regression uses a new type of loss function called insensitive loss function proposed 
by Vapnik:
\begin{equation}
	\label{equ:vapnik}
	E(x,y) = \begin{cases}
		0, & |y-h(x)| \leq \varepsilon \\
		|y-h(x)|-\epsilon, & |y-h(x)| > \varepsilon \\
	\end{cases},
\end{equation}
where $\varepsilon$ is \textbf{TODO: CO TO KWA JEST!}.
The empirical risk is:
\begin{equation}
	\label{equ:risk}
	R_{EMP} = \frac{1}{n}\sum_{i=1}^{n}E(x^{(i)}, y^{(i)}),
\end{equation}
where $n$ is observation count.
SVM regression performs linear regression in the high-dimension feature space using 
$\varepsilon$-insensitive loss and, at the same time, tries to reduce model complexity 
by minimizing $\Vert \Theta \Vert^2$. 
This can be described by introducing (non-negative) slack variables 
$\zeta_{i}, \zeta^{\bullet}_{i}$, to measure the deviation of training samples outside
$\varepsilon$-insensitive zone. Thus SVM regression is formulated as minimization 
of the following functional:
\begin{equation}
	\label{equ:svm_functional_pt1}
	\frac{1}{2}\Vert \Theta \Vert^{2} + C\sum_{i=1}^{n}(\zeta_{i}+\zeta^{\bullet}_{i}),
\end{equation}
where \textbf{TODO: WHAT IS THAT (functional)}:
\begin{equation}
	\label{equ:svm_functional_pt2}
	\begin{cases}
		y_{i}-h(x_{i}) \leq \varepsilon + \zeta^{\bullet}_{i}\\
		h(x_{i}) - y_{i} \leq \varepsilon + \zeta_{i}\\
		\zeta_{i},\zeta^{\bullet}_{i} \geq 0
	\end{cases}.
\end{equation}

This optimization problem can transformed into the dual problem and its solution is given by:
\begin{equation}
	\label{equ:svm_dual}
	f(x)=\sum_{i=1}{v}(\alpha_{i}-\alpha^{\bullet}_{i})K(x_{i},x),
\end{equation}
where $v$ is number of support vectors and the $K(x_{i},x)$ is a kernel function such that:
\begin{equation}
	\label{equ:svm_kernel}
	K(x_{i},x) = \sum_{j=1}^{m}g_{j}(x)g_{j}(x_{i}).
\end{equation}

It is well known that SVM generalization performance (estimation accuracy) depends on a 
good setting of meta-parameters parameters $\varepsilon C$,  and the kernel parameters.
The problem of optimal parameter selection is further complicated by the fact that SVM model
complexity (and hence its generalization performance) depends on all three parameters. 
Existing software implementations of SVM regression usually treat SVM meta-parameters 
as user-defined inputs. Selecting a particular kernel type and kernel function parameters 
is usually based on application-domain knowledge and also should reflect distribution of 
input (x) values of the training data.
\begin{figure}[htb] 
	\label{fig:svm_demo1}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/svm_demo1}
	\caption{Performance of Support Vector Machine in regression case. 
	The epsilon boundaries are given with the green lines. Blue points represent data instances.}
\end{figure}

Parameter $C$ determines the trade off between the model complexity (flatness) and the degree 
to which deviations larger than $\varepsilon$  are tolerated in optimization formulation.
For example, if $C$ is too large (infinity), then the objective is to minimize the empirical 
risk only, without regard to model complexity part in the optimization formulation.
Parameter controls the width of the $\varepsilon$-insensitive zone, used to fit the training data.
The value of $\varepsilon$ can affect the number of support vectors used to construct the
regression function. 
The bigger $\varepsilon$ , the fewer support vectors are selected. On the other hand, 
bigger $\varepsilon$-values results in more ‘flat’ estimates.
Hence, both $C$ and $\varepsilon$-values affect model complexity (but in a different way).
\begin{figure}[htb] 
	\label{fig:svm_demo2}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/svm_demo2}
	\caption{Performance of Support Vector Machine in regression case.
	The epsilon boundaries are given with the green lines. Blue points represent data instances.}
\end{figure}

Additional instances are introduced in this case and after being supplied to the model are 
inside epsilon band. The regression function has changed. 
However this makes some points that were initially inside the interval to be misclassified.
\begin{figure}[htb] 
	\label{fig:svm_demo3}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/svm_demo3}
	\caption{Performance of Support Vector Machine in regression case.
	The epsilon boundaries are given with the green lines. Blue points represent data instances}
\end{figure}
Introducing new data instances that are located inside the epsilon band,
do not influence the structure of the model.
It can be seen that regression function has not changed at all.
One of the advantages of Support Vector Machine, and Support Vector Regression as the part of it,
is that it can be used to avoid difficulties of using linear functions in the high dimensional 
feature space and optimization problem is transformed into dual convex quadratic programmes.
In regression case the loss function is used to penalize errors that are grater than threshold 
- $\varepsilon$.
Such loss functions usually lead to the sparse representation of the decision rule,
giving significant algorithmic and representational advantages. 

%====================================================================================================
\section{Gradient based optimization}
\label{sec:gradient}
How can you minimize a function $\ell$ if you don't know much about it? 
The trick is to assume it is much simpler than it really is. This can be done with 
Taylor's approximation. Provided that the norm $\Vert s \Vert^{2}$ is small 
(i.e. $w + s$ is very close to $w$), we can approximate the function $\ell (w+s)$ by its 
first: 
\begin{equation}
	\label{equ:first_and_second_derivative}
	\ell(w+s) \approx \ell(w) + g(w)^{\top}s,
\end{equation}
end second derivative:
\begin{equation}
	\label{equ:first_and_second_derivative}
	\ell(w+s) \approx \ell(w) + g(w)^{\top}s + \frac{1}{2}s^{\top}H(w)s,
\end{equation}
where $g(w)=\nabla \ell (w)$ is the gradient and $H(w)=\nabla 2 \ell (w)$ 
is the Hessian of $\ell$.
Both approximations are valid if $\Vert s\Vert^{2}$ is small, but the second one assumes that
$\ell$ is twice differentiable and is more expensive to compute but also more accurate 
than only using gradient. 

%----------------------------------------------------------------------------------------------------
\subsection{Gradient descent}
In gradient descent we only use the gradient (first order). 
In other words, we assume that the function $\ell$ around $w$ is linear and behaves like 
$\ell(w)+g(w)^{\top}s$. Our goal is to find a vector $s$ that minimizes this function. 
In steepest descent we simply set
\begin{equation}
	\label{equ:delta_weights}
	s=-\alpha g(w),
\end{equation}
where $\alpha$ is learning rate.
Setting the learning rate $\alpha$ is a dark art. Only if it is sufficiently small will gradien
t descent converge (see the first figure below).
If it is too large the algorithm can easily diverge out of control (see the second figure below).
A safe (but sometimes slow) choice is to set $\alpha = \frac{t_{0}}{t}$, which guarantees that it
will eventually become small enough to converge (for any initial value $t_{0}>0$). 
\begin{figure}[htb] 
	\label{fig:gradient_converge}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/gradient_converge}
	\caption{Gradient descent converging on minimum}
\end{figure}
\begin{figure}[htb] 
	\label{fig:gradient_skip}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/gradient_skip}
	\caption{Gradient descent converging on minimum}
\end{figure}


%----------------------------------------------------------------------------------------------------
\subsection{Momentum based gradient descent}
The problem with gradient descent is that the weight update at a moment is governed by the
learning rate and gradient at that moment only. 
It doesn’t take into account the past steps taken while traversing the cost space.
It leads to the following problems.
\begin{enumerate}
	\item The gradient of the cost function at saddle points( plateau) is negligible or zero,
		which in turn leads to small or no weight updates. 
		Hence, the network becomes stagnant, and learning stops
    \item The path followed by Gradient Descent is very jittery even when 
		operating with mini-batch mode
\end{enumerate}
Consider the below cost surface.
\begin{figure}[htb] 
	\label{fig:gradient_problem}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/gradient_problem}
	\caption{Gradient will have problems}
\end{figure}
Let’s assume the initial weights of the network under consideration correspond to point A. 
With gradient descent, the Loss function decreases rapidly along the slope AB as the gradient
along this slope is high.
But as soon as it reaches point B the gradient becomes very low. The weight updates around B 
is very small. Even after many iterations, the cost moves very slowly before getting stuck at a
point where the gradient eventually becomes zero.
In this case, ideally, cost should have moved to the global minima point C,
but because the gradient disappears at point B, we are stuck with a sub-optimal solution.
Now, Imagine you have a ball rolling from point A. The ball starts rolling down slowly and gathers
some momentum across the slope AB. 
When the ball reaches point B, it has accumulated enough momentum to push itself across the
plateau region B and finally following slope BC to land at the global minima C.
To account for the momentum, we can use a moving average over the past gradients.
In regions where the gradient is high like AB, weight updates will be large. Thus, in a way we
are gathering momentum by taking a moving average over these gradients.
But there is a problem with this method, it considers all the gradients over iterations with 
equal weightage. The gradient at $t=0$ has equal weightage as that of the gradient at
current iteration $t$. We need to use some sort of weighted average of the past gradients such
that the recent gradients are given more weightage.
This can be done by using an Exponential Moving Average(EMA).
An exponential moving average is a moving average that assigns a greater weight on the most 
recent values. The EMA for a series Y may be calculated recursively:
\begin{equation}
	\label{equ:ema}
	S(t) = /beta S(t-1)+(1-\beta)Y(t),
\end{equation}
where the coefficient $\beta \in <0,1>$ represents the degree of weighting increase,  Y(t) is
the value at a period t and  S(t) is the value of the EMA at any period t.
First algorithm step is assumed at $t=1$ and $S(0)=0$.
In our case of a sequence of gradients, the new weight update equation at iteration $t$ becomes:
\begin{equation}
	\label{equ:momentum_update}
	v(t) = \beta v(t-1)+(1-\beta)\delta(t),
\end{equation}
where $v(t)$ is the new weight update done at iteration $t$ $\beta$ is momentum constant and
$\delta(t)$: is the gradient at iteration $t$.
With assumption that $v(0)=0$ change of waights at step $n$ can be described as:
\begin{equation}
	\label{equ:momentum_update_sum}
	v(n) = (1-\beta)\sum_{t=1}^{n}\beta^{n-1}\delta(t).
\end{equation}
In the cost surface shown earlier let's zoom into point C.
With gradient descent, if the learning rate is too small, the weights will be updated very 
slowly hence convergence takes a lot of time even when the gradient is high. 
This is shown in the left side image below. If the learning rate is too high cost
oscillates around the minima as shown in the right side image below.

Let's look at the last summation equation of the momentum again.
When all the past gradients have the same sign the summation term will become large and we 
will take large steps while updating the weights.
Along the curve BC, even if the learning rate is low, all the gradients along the curve will 
have the same direction(sign) thus increasing the momentum and accelerating the descent.
When some of the gradients have $+ve$ sign whereas others have $-ve$ the summation term will
become small and weight updates will be small. 
If the learning rate is high, the gradient at each iteration around the valley C will
alter its sign between $+ve$ and $-ve$ and after few oscillations, the sum of past gradients 
will become small. Thus, making small updates in the weights from there on and damping the 
oscillations.

This to some amount addresses our second problem. Gradient Descent with Momentum takes small 
steps in directions where the gradients oscillate and take large steps along the direction where
the past gradients have the same direction(same sign).

%----------------------------------------------------------------------------------------------------
\subsection{Stochastic gradient descent}
Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an
objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable).
It can be regarded as a stochastic approximation of gradient descent optimization, since it 
replaces the actual gradient (calculated from the entire data set) by an estimate thereof 
(calculated from a randomly selected subset of the data). Especially in high-dimensional 
optimization problems this reduces the computational burden,
achieving faster iterations in trade for a lower convergence rate.
While the basic idea behind stochastic approximation can be traced back to the Robbins–Monro 
algorithm of the 1950s, stochastic gradient descent has become an important optimization
method in machine learning. 
In stochastic (or "on-line") gradient descent, the true gradient  is approximated by a gradient
at a single example.
As the algorithm sweeps through the training set, it performs the above update for each training
example. Several passes can be made over the training set until the algorithm converges.
If this is done, the data can be shuffled for each pass to prevent cycles.
Typical implementations may use an adaptive learning rate so that the algorithm converges

%----------------------------------------------------------------------------------------------------
\subsection{Adaptive gradient algorithm (AdaGrad)}
AdaGrad is simply just an optimization method based off of the Proximal Point Algorithm 
(otherwise known as the Gradient Descent algorithm), specifically the Stochastic version
of gradient descent.
The intention behind the formulation of AdaGrad is because SGD (stochastic gradient descent) 
converges slowly in the cases when features of our model are significantly more 
important than others. 
This is because the original SGD algorithm treats all factors equally, regardless of how much
each factor contributes towards the actual models outputs.

Seen below, we can recognize that the stochastic gradient descent algorithm takes a large
amount of time in order to find the optimal solution because of the constant jumping back and
forth on the expected gradient (seen Left). 
On the otherhand, if we were to average all of the iterates in SGD (not AdaGrad), 
we can find a much quicker optimization and decrease the amount of backtracking done.

Now consider a case where we have an extremely wide problem domain, most likely due to a
feature having an exaggerated effect on the domain.
Seen below, we can see that the standard proximal gradient method (SGD) ends up jumping back and 
forth because it is a poorly conditioned problem.
On the other hand, we can see the AdaGrad variant (which somewhat fixes the poor conditioning of 
the problem) will end up getting to the optimal in a significantly smaller number of steps.
The original SGD algorithm’s update method defines the update rule as:
\begin{equation}
	\label{equ:SGD_equ}
	x^{(k+1)} = argmin_{x\in \chi}\{\langle g^{(k)},x\rangle + frac{1}{2\alpha_{k}}
	\Vert x-x^{(k)}\Vert_{2}^{2}\}.
\end{equation}
This version of the algorithm has a general convergence rate of $O(\frac{1}{T})$.
AdaGrad takes advantage of the matrix norm, in our case, $B$ will be a positive
definite matrix such that:
\begin{equation}
	\label{equ:definite_matrix}
	\Vert  x \Vert_{B}^{2} = x^{T}Bx \geq 0.
\end{equation}
This matrix $B$ has a list of diagonal terms that will modify the update rule of SGD such that:
\begin{equation}
	\label{equ:modified_SGD}
	x^{(k+1)} = argmin_{x\in \chi}\{\langle \nabla f(x^{(k)} ,x\rangle + frac{1}{2\alpha_{k}}
	\Vert x-x^{(k)}\Vert_{B}^{2}\},
\end{equation}
which is equivalent to an update rule of:
\begin{equation}
	\label{equ:mod_sgd_equ}
	x^{(k+1)} = x^{(k)} - \alpha B^{-1} \nabla f(x^{(k)}).
\end{equation}

As can be seen when compared to the original update rule, we have introduced the diagonal matrix
$B$ in order to converge more quickly because of the reliance on features that are
more exaggerated than less important features.
Now the question is how we can best approximate $B$, since it is unknown to the user.
AdaGrad now extends the SGD algorithm by defining within iteration $k$:
\begin{equation}
	\label{equ:adagrad_extension}
	G^{(k)} = diag[\sum_{i=1}^{k}g^{(i)}(g^{(i)})^{T}]^{1/2},
\end{equation}
We can define $G^{(k)}$ as a diagonal matrix with entries:
\begin{equation}
	\label{equ:adagrad_matrix}
	G^{(k)}_{jj} = \sqrt{\sum_{i=1}^{k} (g_j^{(i)})^2}
\end{equation}
Therefore modifying our previous modified update rule to be our final update rule for Adagrad:
\begin{equation}
	\label{equ:adagrad_opt}
	x^{(k+1)} = \text{argmin}_{x \in \chi} \{ \langle \nabla f(x^{(k)}), x \rangle + 
	\frac{1}{2 \alpha_k} \lvert \lvert x - x^{(k)} \rvert \rvert_{G^{(k)}}^2 \}
\end{equation}
which is equivalent to an update rule of:
\begin{equation}
	\label{equ:adagrad_opt_equiv}
	x^{(k+1)} = x^{(k)} - \alpha G^{-1} \nabla f(x^{(k)})
\end{equation}

%----------------------------------------------------------------------------------------------------
\subsection{RMSProp}
RMSprop— is unpublished optimization algorithm designed for neural networks, first proposed by
Geoff Hinton in lecture 6 of the online course “Neural Networks for Machine Learning”.
RMSprop lies in the realm of adaptive learning rate methods, which have been growing in
popularity in recent years, but also getting some criticism. 
It’s famous for not being published, yet being very well-known; most deep learning framework 
include the implementation of it out of the box.

There are two ways to introduce RMSprop. First, is to look at it as the adaptation of rprop
algorithm for mini-batch learning. 
It was the initial motivation for developing this algorithm. Another way is to look at its 
similarities with Adagrad and view RMSprop as a way to deal with its radically diminishing
learning rates.
I’ll try to hit both points so that it’s clearer why the algorithm works.

Let’s start with understanding rprop — algorithm that’s used for full-batch optimization. 
Rprop tries to resolve the problem that gradients may vary widely in magnitudes.
Some gradients may be tiny and others may be huge, which result in very difficult problem —
trying to find a single global learning rate for the algorithm. 
If we use full-batch learning we can cope with this problem by only using the sign 
of the gradient.
With that, we can guarantee that all weight updates are of the same size. 
This adjustment helps a great deal with saddle points and plateaus as we take big enough steps
even with tiny gradients. 
Note, that we can’t do that just by increasing the learning rates, because steps we take with 
large gradients are going to be even bigger, which will result in divergence.
Rprop combines the idea of using the sign of the gradient with the idea of adapting step size
individualy for each weight.
So, instead of looking at the magnitude of the gradient, we’ll look at the step size that’s 
defined for that particular weight.
And that step size adapts individually over time, so that we accelerate learning in the
direction that we need. To adjust the step size for some weight, the following algorithm is used
\begin{enumerate}
    \item First, we look at the signs of the last two gradients for the weight.
    \item If they have the same sign, that means, we’re going in the right direction,
		and should accelerate it by a small fraction, meaning we should increase the step size
		multiplicatively(e.g by a factor of 1.2). 
		If they’re different, that means we did too large of a step and jumped over a
		local minima, thus we should decrease the step size multiplicatively(e.g. by a
		factor of 0.5).
    \item Then, we limit the step size between some two values. These values really depend on
		your application and dataset, good values that can be for default are 50 and a millionth, 
		which is a good start.
    \item Now we can apply the weight update.
\end{enumerate}

Rprop doesn’t really work when we have very large datasets and need to perform mini-batch weights
updates. 
Why it doesn’t work with mini-batches ? Well, people have tried it, but found it hard to make 
it work.
The reason it doesn’t work is that it violates the central idea behind stochastic gradient
descent, which is when we have small enough learning rate,
it averages the gradients over successive mini-batches.
Consider the weight, that gets the gradient 0.1 on nine mini-batches, and the gradient of -0.9 on
tenths mini-batch.
What we’d like is to those gradients to roughly cancel each other out, so that the stay
approximately the same.
But it’s not what happens with rprop. With rprop, we increment the weight 9 times and decrement
only once, so the weight grows much larger.

To combine the robustness of rprop (by just using sign of the gradient), efficiency we get 
from mini-batches, and averaging over mini-batches which allows to combine gradients in the
right way, we must look at rprop from different perspective.
Rprop is equivalent of using the gradient but also dividing by the size of the gradient,
so we get the same magnitude no matter how big a small that particular gradient is.
The problem with mini-batches is that we divide by different gradient every time, so why not force
the number we divide by to be similar for adjacent mini-batches ?
The central idea of RMSprop is keep the moving average of the squared gradients for each weight.
And then we divide the gradient by square root the mean square. 
Which is why it’s called RMSprop(root mean square). With math equations the update rule
looks like this:
\begin{equation}
	\label{equ:rmsprop1}
	E[g^{2}]_{t} = \beta E[g^{2}]_{t-1}+(1-\beta)(\frac{\delta C}{\delta w})^{2},
\end{equation}
\begin{equation}
	\label{equ:rmsprop2}
	w_{t} = w_{t-1} - \frac{\eta}{\sqrt{E[g^{2}]_{t}}} \frac{\delta C}{\delta w},
\end{equation}
where $E[g]$ is moving average of squared gradients. $\frac{\delta C}{\delta w}$ is gradient of
the cost function with respect to the weight, $\eta$  learning rate and $\beta$  moving average
parameter.
As you can see from the above equation we adapt learning rate by dividing by the root of
squared gradient, but since we only have the estimate of the gradient on the current mini-batch,
wee need instead to use the moving average of it.
%----------------------------------------------------------------------------------------------------
\subsection{Adam}
%----------------------------------------------------------------------------------------------------
\subsection{Newton's Method}
Newton's method assumes that the loss $\ell$ is twice differentiable and uses the approximation 
with Hessian (2nd order Taylor approximation).
The Hessian Matrix contains all second order partial derivatives and is defined as:
\begin{equation}
	\label{equ:hessian}
	H(w) = \left( \begin{array}{c c c c}
		\frac{\partial^{2}\ell}{\partial w_{1}^{2}}&
		\frac{\partial^{2}\ell}{\partial w_{1}w_{2}}& \cdots&
		\frac{\partial^{2}\ell}{\partial w_{1}w_{n}}\\

		\frac{\partial^{2}\ell}{\partial w_{2}w_{1}}&
		\frac{\partial^{2}\ell}{\partial w_{2}^{2}}& \cdots&
		\frac{\partial^{2}\ell}{\partial w_{2}w_{n}}\\

		\vdots& \vdots& \ddots& \hdots\\

		\frac{\partial^{2}\ell}{\partial w_{n}w_{1}}&
		\frac{\partial^{2}\ell}{\partial w_{n}w_{2}}& \cdots&
		\frac{\partial^{2}\ell}{\partial w_{n}^{2}}\\
	\end{array} \right),
\end{equation}
because the convexity of $\ell$, it is always a symmetric square matrix and positive
semi-definite. 
A symmetric matrix $M$ is positive semi-definite if it has only non-negative eigenvalues or, 
equivalently, for any vector $x$ we must have $x^{T}Mx\geq 0$.

It follows that the approximation
\begin{equation}
	\label{equ:newton_approx}
	\ell(w+s) \approx \ell(w) + g(w)^{T}s + 0.5s^{T}H(w)s,
\end{equation}
describes a convex parabola, and we can find its minimum by solving the following 
optimization problem: 
\begin{equation}
	\label{equ:newton_opt}
	argmin[\ell(w) + g(w)^{T}s + 0.5s^{T}H(w)s].
\end{equation}
To find the minimum of the objective, we take its first derivative with respect to $s$,
equate it with zero, and solve for $s$:
\begin{equation}
	\label{equ:newton_opt_solved}
	g(w) + H(w)s = 0 \to s = -H^{-1}(w)g(w).
\end{equation}
This choice of s converges extremely fast if the approximation is sufficiently accurate and the 
resulting step sufficiently small.
Otherwise it can diverge. Divergence often happens if the function is flat or almost flat with 
respect to some dimension. In that case the second derivatives are close to zero, and their
inverse becomes very large - resulting in gigantic steps.
Different from gradient descent, here there is no step-size that guarantees that steps are 
all small and local. 
As the Taylor approximation is only accurate locally, large steps can move the current estimates
far from regions where the Taylor approximation is accurate. 
\begin{figure}[htb] 
	\label{fig:gradient_newton}
	\centering
	\includegraphics[width=\textwidth]{figures/gradient_newton}
	\caption{A gradient descent step (left) and a Newton step (right) on the same function}
\end{figure}
To avoid divergence of Newton's method, a good approach is to start with gradient descent 
(or even stochastic gradient descent) and then finish the optimization Newton's method.
Typically, the second order approximation, used by Newton's Method, is more likely to be
appropriate near the optimum.

%====================================================================================================
\section{Neural networks}

%----------------------------------------------------------------------------------------------------
\subsection{Artificial representation of biological neuron}
A neuron or nerve cell is an electrically excitable cell that communicates with other
cells via specialized connections called synapses. 
Neurons are typically classified into three categories based on their function:
\begin{enumerate}
	 \item sensory neurons that respond to stimuli and they send signals to the spinal cord or
		 directly to brain, 
	\item motor neurons that receive signals from the brain and spinal cord to control body
		effectors like muscles or glandular output,
	\item interneurons that connect neurons to other neurons within the same region of the brain 
		or spinal cord.
\end{enumerate}
As neuron is first and foremost a cells it consists of a soma (cell body), dendrites, and a 
single axon. The soma is usually compact. The axon and dendrites are filaments that extrude 
from it. Dendrites typically branch profusely and extend a few hundred micrometers from the soma.
The axon leaves the soma at a swelling called the axon hillock, and travels for as far as 1 meter 
in humans or more in other species. It branches but usually maintains a constant diameter.
At the farthest tip of the axon's branches are axon terminals, where the neuron can transmit
a signal across the synapse to another cell.
Neurons may lack dendrites or have no axon. The term neurite is used to describe either a dendrite
or an axon, particularly when the cell is undifferentiated.

Most neurons receive signals via the dendrites and soma and send out signals down the axon.
At the majority of synapses, signals cross from the axon of one neuron to a dendrite of another.
However, synapses can connect an axon to another axon or a dendrite to another dendrite.
\begin{figure}[htb] 
	\label{fig:neurons}
	\centering
	\includegraphics[width=\textwidth]{figures/bio_neurons}
	\caption{Examples of biological neurons}
\end{figure}

The signaling process is partly electrical and partly chemical. Neurons are electrically 
excitable, due to maintenance of voltage gradients across their membranes. 
If the voltage changes by a large enough amount over a short interval, 
the neuron generates an all-or-nothing electrochemical pulse called an action potential. 
This potential travels rapidly along the axon, and activates synaptic connections as it 
reaches them. Synaptic signals may be excitatory or inhibitory, 
increasing or reducing the net voltage that reaches the soma.
\begin{figure}[htb] 
	\label{fig:bio_activation}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/bio_activation}
	\caption{Examples of biological neurons}
\end{figure}

In 1943 Warren S. McCulloch, a neuroscientist, and Walter Pitts, a logician, 
published "A logical calculus of the ideas immanent in nervous activity" in the 
Bulletin of Mathematical Biophysics 5:115-133. In this paper McCulloch and Pitts tried to
understand how the brain could produce highly complex patterns by using many basic cells
that are connected together. 
McCulloch Pitts neuron is a highly simplified model of neural cell that models only a most
basic behavior of electrical signal transmition.
Neuron is divided into three blocks:
\begin{enumerate}
	\item aggregation where many input signals $\{x_{1}, x_{2}, \cdots, x_{n}\}$ are 
		aggregated into single signal $\hat{u}$,
	\item dampening where signal strength is reduced according to bias term and dampened 
		response $u$ is returned,
	\item activation where signal $u$ is transformed to neuron response space.
\end{enumerate}
Response of neuron modelled like that can be written as:
\begin{equation}
	\label{equ:neuron_response}
	h_{x} = f_{a}(u(\hat{u}(x))),
\end{equation}
however as in most cases weighted sum is realisation of aggregation and dampening is handled by
a simple substation neuron equation is simplified to: 
\begin{equation}
	\label{equ:neuron_response}
	h_{x} = f_{a}(u(x)).
\end{equation}
In case of McCulloch Pitts neuron both aggregation and dampening are realised by a single 
linear function akin to linear predictor:
\begin{equation}
	\label{equ:linear_neuron}
	u(x) = \Theta_{0} + \Theta_{1}x_{1} + \cdots + \Theta_{n}x_{n},
\end{equation}
which as described in chapter \ref{sec:linear_regression} can be written in a matrix notation as
\begin{equation}
	\label{equ:linear_neuron_matrix}
	u(X) = \Theta^{T}\cdot X.
\end{equation}
Almost all modern implementations of artificial neural networks use this aggregation, dampening
model and the only difference between them is in an activation function.
McCulloch Pitts neuron uses step function as an activation:
\[
	\label{equ:step_function}
	 f_{STEP}(u)=
	\begin{cases}
		1,&  u > 0, \\
		0,&  u \leq 0,
	\end{cases}
	.
\]
With this function neuron can process signals however it still lacks a very important function
of a biological neuron, ability to learn. First learning algorithm developed on base of 
McCulloch Pitts neuron was perceptron algorithm created by \textbf{TODO: BY WHO}.
With $\hat{y}$ defined as a neuron prediction and $y$ as a expected output perceptron learning
algorithm can me described as:
\begin{enumerate}
	\item if $\hat{y}=y$ then do nothing,
	\item if $\hat{y}=1$ and $y=0$ then $\Theta \leftarrow \Theta + x$
	\item if $\hat{y}=0$ and $y=1$ then $\Theta \leftarrow \Theta - x$
\end{enumerate}
Due to its ability to separate space into two subspaces perceptron can be used to model a logical
operators and as such realise logical reasoning.
Let's start with the AND operator.  Looking back at the logic table for the $A\land B$, 
we can see that we only want the neuron to output a 1 when both inputs are activated.
To do this, we want the sum of both inputs to be greater than the threshold, but each input alone
must be lower than the threshold.  Let's use a threshold of 1. 
So, now we need to choose the weights according to the constraints I've just explained - 
how about 0.6 and 0.6?  With these weights, individual activation of either input A or B will 
not exceed the threshold, while the sum of the two will be 1.2, which exceeds the threshold 
and causes the neuron to fire.
I've used a greek letter theta to denote the threshold, which is quite common practice. 
A and B are the two inputs.  You can think of them as input neurons, like photoreceptors, 
taste buds, olfactory receptors, etc.  They will each be set to 1 or 0 depending upon the truth of
their proposition.  The red neuron is our decision neuron. 
If the sum of the synapse-weighted inputs is greater than the threshold, 
this will output a 1, otherwise it will output a 0.  
So, to test it by hand, we can try setting A and B to the different values in the 
truth table and seeing if the decision neuron's output matches the $A\land B$ column:
\begin{itemize}
	\item  $A=0 \land B=0  \to   \hat{y} = 0*1  +  0*1 - 1  = -1 \to$,
	\item $A=0 \land B=1  \to  \hat(y) = 0*1  +  1*1  = 0.6$,
	\item $A=1 \land B=0  \to  \hat(y) = 1*1  +  0*1  = 0.6$,
	\item $A=1 \land B=1  \to  \hat(y) = 1*1  +  1*1  = 1.2$.
\end{itemize}

We have designed a neuron which implements a logical AND gate.  

This is easy to implement in Excel.  In fact, it's exactly the same as the neuron we 
created in What does a neuron do.  

Next up is the OR gate.  Look back at the logic table.  In this case, we want the output to 
be 1 when either or both of the inputs, A and B, are active, but 0 when both of the inputs are 0.  
This is simple enough.  If we make each synapse greater than the threshold, 
then it'll fire whenever there is any activity in either or both of A and B.
This is shown on the right.  Synaptic values of 1.1 are sufficient to surpass 
the threshold of 1 whenever their respective input is active.  
	

%----------------------------------------------------------------------------------------------------
\subsection{Feed forward neural networks}

\begin{figure}[htb] 
	\label{fig:xor_gates}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/xor_gates}
	\caption{Implementation of XOR gate with AND, OR and NOT gates}
\end{figure}

\begin{figure}[htb] 
	\label{fig:neuro_xor}
	\centering
	\includegraphics[width=\textwidth]{figures/neuro_xor}
	\caption{Neuron based equivalent of XOR function}
\end{figure}

%----------------------------------------------------------------------------------------------------
\subsection{Simple recurrent neural networks}
Simple solution to problem of time independence is to concatenate response of neural layer
from previous cycle to it input 
\begin{equation}
	\label{equ:sru_input}
	x'(t)=[x(t)|y(t-1)]
\end{equation}
Such solution results in signal propagating trough time and influencing responses of future cycles,
if this is only modification to feed forward model such layer is called simple recurrent
unit (SRU).
While this solution makes model time aware it have its own problems, mainly a signal vanishing
issue. Since the input signal from cycle $n$ have direct influence only on a response of this
cycle and for each subsequent cycles it is only trough feedback loop. Influence of input $n$ on
response of cycle $n+k$ grows inverse proportional to $k$.
This means that in this model only those regularities that appear over short time periods can
be detected.
Making weights on feedback bigger will not eliminate problem and instead replace it with signal
explosion that causes response to reach maximum value if a strong signal appeared on input at
least once.

%----------------------------------------------------------------------------------------------------
\subsection{Networks with long term memory}
One of possible solutions to this issue is addition of long term memory which will regulate
forward and loop back path influence on neuron response, such solution is used in long short
term memory (LSTM) networks \cite{Hochreiter1997}.
\begin{figure}[htb] 
	\label{fig:lstm}
	\includegraphics[width=\textwidth]{figures/lstm}
	\caption{LSTM layer}
\end{figure}
Single LSTM neuron consists of 4 basic neurons and two non neuron operations:
\begin{itemize}
\item $x'(t)=[x(t)|y(t-1)]$,
\item $o_t=\sigma (W_o\cdot x'_t+b_o)$,
\item $r_t=\sigma (W_r\cdot x'_t+b_r)$,
\item $f_t=\sigma (W_f\cdot x'_t+b_f)$,
\item $\bar{C}_t=\tanh (W_c\cdot x'_t+b_c)$,
\item $C_t=f_t\circ C_{t-1}+r_t\circ \bar{C}_t$,
\item $y_t=o_t\circ \tanh (C_t)$.
\end{itemize}
With $W$ and $b$ being weights and biases for each basic neuron, $x$ input, $y$ output and
$C$ long term memory. As it can be seen $o_t$ is a equivalent of SRU and is moderated by
long term memory before propagating as output. Temporary value of long term memory based
only on current output $\bar{C}_t$ is calculated and then with help of neurons $r$ and $f$
is transformed into its final value.
Neuron $r$ is called remembering gate and influences to what degree temporary long term
memory from given cycle effects its final value while $f$ is forgetting gate and
decides influence of long term memory from last cycle on current one.
Thanks to such implementation model can learn to detect long term regularities as well
as short term ones.

%====================================================================================================
\section{Regularization}

%----------------------------------------------------------------------------------------------------
\subsection{Early stopping}
%----------------------------------------------------------------------------------------------------
\subsection{$\ell$ regularization}
%----------------------------------------------------------------------------------------------------
\subsection{Dropout}
