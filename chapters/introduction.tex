\chapter{Introduction}
FIXME : Textwidth in cm: \printinunitsof{cm}\prntlen{\textwidth}

%==================================================================================================
\section{Motivation}
I have encountered a problem that was insipration for this work during my research in field
of mobile robotics. While my initial focus was directed more towards emergent behaviour in 
robotics and a self organizing systems I have encountered a issue with localization in marine 
robotics. With high cost of internet connection for robots operating far away from land it 
is important to be able to calculate precise position without updating data from internet.
One of issuse when working with satellite navigation system is requirement for clock bias
correction, to keep error drift to minimum readouts for both local and satellite clock must
be corrected by predicted bias. In this work I focused on prediction of errors in satellite
clocks as unlike in case of local clock those can be later reused by other people.


%==================================================================================================
\section{Beacon based navigation in robotics}

%--------------------------------------------------------------------------------------------------
\subsection{Baisc concepts in robot localisation}
Navigation is one of the most challenging competences required of a mobile robot. Success
in navigation requires success at the four building blocks of navigation: perception, the
robot must interpret its sensors to extract meaningful data; localization, the robot must
determine its position in the environment (figure 5.1); cognition, the robot must decide how
to act to achieve its goals; and motion control, the robot must modulate its motor outputs to
achieve the desired trajectory.
Of these four components (figure 5.2), localization has received the greatest research
attention in the past decade and, as a result, significant advances have been made on this
front. In this chapter, we explore the successful localization methodologies of recent years.
First, section 5.2 describes how sensor and effector uncertainty is responsible for the 
difficulties of localization. Then, section 5.3 describes two extreme approaches to dealing with
the challenge of robot localization: avoiding localization altogether, and performing
explicit map-based localization. The remainder of the chapter discusses the question of
representation, then presents case studies of successful localization systems using a variety of
representations and techniques to achieve mobile robot localization competence.
There are a wide variety of sensors used in mobile robots (figure 4.1). Some sensors are
used to measure simple values like the internal temperature of a robot’s electronics or the
rotational speed of the motors. Other, more sophisticated sensors can be used to acquire
information about the robot’s environment or even to directly measure a robot’s global
position. In this chapter we focus primarily on sensors used to extract information about the
robot’s environment. Because a mobile robot moves around, it will frequently encounter
unforeseen environmental characteristics, and therefore such sensing is particularly critical.
We begin with a functional classification of sensors. Then, after presenting basic tools for
describing a sensor’s performance, we proceed to describe selected sensors in detail.
We classify sensors using two important functional axes: proprioceptive/exteroceptive and
passive/active.
Proprioceptive sensors measure values internal to the system (robot); for example,
motor speed, wheel load, robot arm joint angles, battery voltage.
Exteroceptive sensors acquire information from the robot’s environment; for example,
distance measurements, light intensity, sound amplitude. Hence exteroceptive sensor measurements 
are interpreted by the robot in order to extract meaningful environmental features.
Passive sensors measure ambient environmental energy entering the sensor. Examples
of passive sensors include temperature probes, microphones, and CCD or CMOS cameras.
Active sensors emit energy into the environment, then measure the environmental reaction.
Because active sensors can manage more controlled interactions with the environment,
they often achieve superior performance. However, active sensing introduces several
risks: the outbound energy may affect the very characteristics that the sensor is attempting
to measure. Furthermore, an active sensor may suffer from interference between its signal.
and those beyond its control. For example, signals emitted by other nearby robots, or sim-
ilar sensors on the same robot, may influence the resulting measurements. Examples of
active sensors include wheel quadrature encoders, ultrasonic sensors, and laser rangefinders.
Mobile robots depend heavily on exteroceptive sensors. Many of these sensors concentrate
on a central task for the robot: acquiring information on objects in the robot’s immediate
vicinity so that it may interpret the state of its surroundings. Of course, these ``objects'' 
surrounding the robot are all detected from the viewpoint of its local reference frame. Since
the systems we study are mobile, their ever-changing position and their motion have a 
significant impact on overall sensor behavior. In this section, empowered with the terminology 
of the earlier discussions, we describe how dramatically the sensor error of a mobile robot 
disagrees with the ideal picture drawn in the previous section.
Blurring of systematic and random errors. Active ranging sensors tend to have failure
modes that are triggered largely by specific relative positions of the sensor and environment
targets. For example, a sonar sensor will produce specular reflections, producing grossly
inaccurate measurements of range, at specific angles to a smooth sheetrock wall. During
motion of the robot, such relative angles occur at stochastic intervals. This is especially true
in a mobile robot outfitted with a ring of multiple sonars. The chances of one sonar entering
this error mode during robot motion is high. From the perspective of the moving robot, the
sonar measurement error is a random error in this case. Yet, if the robot were to stop,
becoming motionless, then a very different error modality is possible. If the robot’s static
position causes a particular sonar to fail in this manner, the sonar will fail consistently and
will tend to return precisely the same (and incorrect!) reading time after time. Once the
robot is motionless, the error appears to be systematic and of high precision.
The fundamental mechanism at work here is the cross-sensitivity of mobile robot sensors to robot
pose and robot-environment dynamics. The models for such cross-sensitivity
are not, in an underlying sense, truly random. However, these physical interrelationships
are rarely modeled and therefore, from the point of view of an incomplete model, the errors
appear random during motion and systematic when the robot is at rest.
Sonar is not the only sensor subject to this blurring of systematic and random error
modality. Visual interpretation through the use of a CCD camera is also highly susceptible
to robot motion and position because of camera dependence on lighting changes, lighting
specularity (e.g., glare), and reflections. The important point is to realize that, while 
systematic error and random error are well-defined in a controlled setting, the mobile robot can
exhibit error characteristics that bridge the gap between deterministic and stochastic error
mechanisms.
Multimodal error distributions. It is common to characterize the behavior of a sensor’s
random error in terms of a probability distribution over various output values. In general,
one knows very little about the causes of random error and therefore several simplifying
assumptions are commonly used. For example, we can assume that the error is zero-mean,
in that it symmetrically generates both positive and negative measurement error. We can
go even further and assume that the probability density curve is Gaussian. Although we discuss 
the mathematics of this in detail in section 4.2, it is important for now to recognize the
fact that one frequently assumes symmetry as well as unimodal distribution. This means
that measuring the correct value is most probable, and any measurement that is further
away from the correct value is less likely than any measurement that is closer to the correct
value. These are strong assumptions that enable powerful mathematical principles to be
applied to mobile robot problems, but it is important to realize how wrong these assumptions 
usually are.
Consider, for example, the sonar sensor once again. When ranging an object that reflects
the sound signal well, the sonar will exhibit high accuracy, and will induce random error
based on noise, for example, in the timing circuitry. This portion of its sensor behavior will
exhibit error characteristics that are fairly symmetric and unimodal. However, when the
sonar sensor is moving through an environment and is sometimes faced with materials that
cause coherent reflection rather than returning the sound signal to the sonar sensor, then the
sonar will grossly overestimate the distance to the object. In such cases, the error will be
biased toward positive measurement error and will be far from the correct value. The error
is not strictly systematic, and so we are left modeling it as a probability distribution of
random error. So the sonar sensor has two separate types of operational modes, one in
which the signal does return and some random error is possible, and the second in which
the signal returns after a multipath reflection, and gross overestimation error occurs. The
probability distribution could easily be at least bimodal in this case, and since overestimation 
is more common than underestimation it will also be asymmetric.
As a second example, consider ranging via stereo vision. Once again, we can identify
two modes of operation. If the stereo vision system correctly correlates two images, then
the resulting random error will be caused by camera noise and will limit the measurement
accuracy. But the stereo vision system can also correlate two images incorrectly, matching
two fence posts, for example, that are not the same post in the real world. In such a case
stereo vision will exhibit gross measurement error, and one can easily imagine such behavior 
violating both the unimodal and the symmetric assumptions.
The thesis of this section is that sensors in a mobile robot may be subject to multiple
modes of operation and, when the sensor error is characterized, unimodality and symmetry
may be grossly violated. Nonetheless, as we shall see, many successful mobile robot systems 
make use of these simplifying assumptions and the resulting mathematical techniques
with great empirical success.
The above sections have presented a terminology with which we can characterize the
advantages and disadvantages of various mobile robot sensors. In the following sections,
we do the same for a sampling of the most commonly used mobile robot sensors today.
%--------------------------------------------------------------------------------------------------
\subsection{Radio beacon localisation}
All Global Satellite Navigation systems (GNSS) are variant of beacon-based localization
systems\cite{Blewitt1997}. Such systems require information about the beacon position
and distance between the localized object and beacons.
With that information, it is possible to calculate the position of an object in the same reference
frame as that of beacons.
Both of those tasks are much more difficult in GNSS due to the nature of the beacons.
Unlike in the case of stationary beacons, GNSS satellites move at high speed so
their position must be calculated based on satellite ephemerides.
Another problem is how to measure distance with use of reasonably priced reciever while 
maintaining high level of precision.
There are three possible approaches to a beacon based localisation:
\begin{itemize}
	\item Time of Arrival (ToA),
	\item Angle of Arrival (AoA),
	\item Received Signal Strength (RSS).
\end{itemize}
AoA detects at which angle a beacon signal arrives and reqires a specialised receiver capable 
of such measurements. This type of localization requires a complex reciever and do not provide 
a satisfactionary precision when dealing with such remote objects as a satellites.
While RSS can work with a very simple reciever its precision is low for signals that, like
the GPS signal, are designed with low power loss over large distances.
This leaves ToA as only valid solution, when measuring distance by ToA 
three properties of a signal should be known:
\begin{itemize}
	\item $t_o$ - time of origination,
	\item $t_a$ - time of arrival,
	\item $v$ - velocity.
\end{itemize}
In case of GNSS signal is an electromagnetic wave therefore its speed is equal
to a speed of light $c$. Time of arrival is recorded when
data frame wavefront reaches the receiver, this means that receiver time is used.
Signal generation time is recorded on satellite according to its local clock and
included in the data frame. Thanks to that distance can be calculated by simple
equation:
\begin{equation}
	d=c(t_a-t_o).
\end{equation}
However $t_a$ and $t_o$ are using different reference frame so for comparison
to be possible they must be transformed into a common reference frame.
This is referred to as a synchronization of the clocks and is very important as
a desynchronization on the level of a single nanosecond results in about 30 cm of
positioning error\cite{Enge2011}.

%--------------------------------------------------------------------------------------------------
\subsection{Global navigation satellite systems}





